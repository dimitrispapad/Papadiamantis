import os
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import hdbscan
import umap

# Prepare documents
filenames = [f for f in os.listdir() if f.endswith(".txt")]
documents = [(fname, open(fname, "r", encoding="utf-8").read()) for fname in filenames]
texts = [text for _, text in documents]
names = [name for name, _ in documents]

# Define configurations
embedding_models = {
    "MiniLM": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "DistilUSE": "sentence-transformers/distiluse-base-multilingual-cased-v1",
    "XLM-R": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens"
}
reducers = {
    "none": None,
    "umap": umap.UMAP(n_components=15, metric='cosine', random_state=42)
}
clusterers = {
    "hdbscan": [{"min_cluster_size": mcs, "min_samples": ms} for mcs in [5, 10] for ms in [1, 5]],
    "agglomerative": [{"n_clusters": k} for k in [5, 10, 15]]
}

results = []

# Run experiments
for model_name, model_path in embedding_models.items():
    print(f"\nüîç Embedding with {model_name}")
    model = SentenceTransformer(model_path)
    embeddings = model.encode(texts, show_progress_bar=True)
    norm_embeddings = normalize(embeddings, norm='l2').astype(np.float64)

    for reducer_name, reducer in reducers.items():
        print(f"‚Üí Using reducer: {reducer_name}")
        reduced = reducer.fit_transform(norm_embeddings).astype(np.float64) if reducer else norm_embeddings


        for clusterer_name, param_list in clusterers.items():
            for params in param_list:
                try:
                    print(f"  ‚Ä¢ Clustering with {clusterer_name}, Params: {params}")
                    if clusterer_name == "hdbscan":
                        clusterer = hdbscan.HDBSCAN(**params, metric='euclidean')
                        labels = clusterer.fit_predict(reduced)
                        dbcv = hdbscan.validity.validity_index(reduced, labels)
                    elif clusterer_name == "agglomerative":
                        clusterer = AgglomerativeClustering(**params)
                        labels = clusterer.fit_predict(reduced)
                        dbcv = np.nan  # Not available

                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                    if n_clusters < 2:
                        continue

                    mask = labels != -1
                    sil = silhouette_score(reduced[mask], labels[mask])
                    chs = calinski_harabasz_score(reduced[mask], labels[mask])
                    dbs = davies_bouldin_score(reduced[mask], labels[mask])

                    results.append({
                        "Model": model_name,
                        "Reducer": reducer_name,
                        "Clustering": clusterer_name,
                        "Params": str(params),
                        "Silhouette": round(sil, 3),
                        "Calinski-Harabasz": round(chs, 2),
                        "Davies-Bouldin": round(dbs, 3),
                        "DBCV": round(dbcv, 3) if not np.isnan(dbcv) else np.nan,
                        "Num_Clusters": n_clusters
                    })

                except Exception as e:
                    print(f"‚ùå Error with config: {params} ‚Äî {e}")

# Export results
results_df = pd.DataFrame(results)
results_df.to_csv("clustering_evaluation_summary.csv", index=False)
print("\n‚úÖ Results saved to 'clustering_evaluation_summary.csv'")
print(results_df.sort_values(by="Silhouette", ascending=False).head())
