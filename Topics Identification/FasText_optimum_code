import re
import chardet
import spacy
import numpy as np
import hdbscan
import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict
from scipy.spatial.distance import cosine
from umap import UMAP
from gensim.models import FastText

# Load and preprocess text
with open("Diigimata.txt", 'rb') as file:
    encoding = chardet.detect(file.read())['encoding']
with open("Diigimata.txt", 'r', encoding=encoding) as file:
    raw_text = file.read()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'ς\b', 'σ', text)
    return text

processed_text = preprocess_text(raw_text)

# Load Greek language model
nlp = spacy.load("el_core_news_sm")
nlp.max_length = 3_000_000

# Load stopwords
try:
    with open("stopwords.txt", 'r', encoding='utf-8') as f:
        stopwords = set(line.strip() for line in f)
except:
    stopwords = set()

# Tokenize
doc = nlp(processed_text)
tokenized_sentences = [
    [token.text for token in sent if not token.is_space and not token.is_punct and token.text not in stopwords]
    for sent in doc.sents
]

# Train FastText with best UMAP config
model = FastText(
    sentences=tokenized_sentences,
    vector_size=200,
    window=3,
    min_count=1,
    epochs=30,
    sg=1,
    workers=4,
    seed=42
)

# Select top words
words = list(model.wv.index_to_key)[:8000]
vectors = np.array([model.wv[word] for word in words])

# Reduce dimensions using UMAP
umap_model = UMAP(n_components=2, random_state=42)
reduced = umap_model.fit_transform(vectors)

# Cluster with HDBSCAN
clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5, metric='euclidean')
labels = clusterer.fit_predict(reduced)

# Group words by cluster
cluster_words = defaultdict(list)
for word, label in zip(words, labels):
    if label != -1:
        cluster_words[label].append(word)

# Get 10 representative words per cluster
top_words = {}
for label, words_in_cluster in cluster_words.items():
    vecs = np.array([model.wv[word] for word in words_in_cluster])
    center = vecs.mean(axis=0)
    dists = [(word, np.linalg.norm(model.wv[word] - center)) for word in words_in_cluster]
    top_words[label] = [w for w, _ in sorted(dists, key=lambda x: x[1])[:10]]

# Save to CSV
df_top_words = pd.DataFrame.from_dict(top_words, orient='index').T
df_top_words.to_csv("umap_fasttext_top_words_per_cluster.csv", index=False)
print("✅ Saved cluster top words to umap_fasttext_top_words_per_cluster.csv")

# Plot clusters
plt.figure(figsize=(12, 8))
for label in set(labels):
    if label == -1:
        continue
    idx = np.where(labels == label)
    plt.scatter(reduced[idx, 0], reduced[idx, 1], label=f"Cluster {label}", alpha=0.6)

plt.title("UMAP Word Clusters (FastText)")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.tight_layout()
plt.savefig("umap_fasttext_word_clusters.png")
plt.show()
