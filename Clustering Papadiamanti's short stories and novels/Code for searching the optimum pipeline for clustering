#bestsearch
import os
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import umap
import hdbscan

# Load .txt files
documents = [(f, open(f, 'r', encoding='utf-8').read()) for f in os.listdir() if f.endswith(".txt")]
texts = [text for _, text in documents]
names = [name for name, _ in documents]

# Define embedding models
embedding_models = {
    "MiniLM": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "DistilUSE": "sentence-transformers/distiluse-base-multilingual-cased-v1",
    "XLM-R": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens"
}

# Define reducers
reducers = {
    "none": None,
    "pca": PCA(n_components=15, random_state=42),
    "tsne": TSNE(n_components=2, init="pca", random_state=42),
    "umap": umap.UMAP(n_components=15, metric="cosine", random_state=42),
}

# HDBSCAN grid
hdbscan_grid = [
    {"min_cluster_size": mcs, "min_samples": ms}
    for mcs in [5, 10, 15]
    for ms in [1, 5, 10]
]

# KMeans grid
kmeans_grid = [{"n_clusters": k} for k in [3, 5, 7]]

results = []

# Grid search
for model_name, model_path in embedding_models.items():
    print(f"\nüîç Embedding with {model_name}...")
    model = SentenceTransformer(model_path)
    embeddings = model.encode(texts, show_progress_bar=True)
    norm_embeddings = normalize(embeddings, norm="l2").astype(np.float64)

    for reducer_name, reducer in reducers.items():
        print(f"\nüîß Reducer: {reducer_name}")
        try:
            reduced = reducer.fit_transform(norm_embeddings).astype(np.float64) if reducer else norm_embeddings
        except Exception as e:
            print(f"‚ùå Reducer error: {e}")
            continue

        # HDBSCAN
        for params in hdbscan_grid:
            try:
                clusterer = hdbscan.HDBSCAN(**params, metric='euclidean')
                labels = clusterer.fit_predict(reduced)
                dbcv = hdbscan.validity.validity_index(reduced.astype(np.float64), labels)
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                if n_clusters < 2:
                    continue
                mask = labels != -1
                sil = silhouette_score(reduced[mask], labels[mask])
                db = davies_bouldin_score(reduced[mask], labels[mask])
                results.append({
                    "Model": model_name,
                    "Reducer": reducer_name,
                    "Clustering": "HDBSCAN",
                    "Params": str(params),
                    "Silhouette": round(sil, 3),
                    "Davies-Bouldin": round(db, 3),
                    "DBCV": round(dbcv, 3),
                    "Num_Clusters": n_clusters
                })
            except Exception as e:
                print(f"‚ùå HDBSCAN failed: {e}")

        # KMeans
        for params in kmeans_grid:
            try:
                clusterer = KMeans(**params, random_state=42, n_init=10)
                labels = clusterer.fit_predict(reduced)
                sil = silhouette_score(reduced, labels)
                db = davies_bouldin_score(reduced, labels)
                results.append({
                    "Model": model_name,
                    "Reducer": reducer_name,
                    "Clustering": "KMeans",
                    "Params": str(params),
                    "Silhouette": round(sil, 3),
                    "Davies-Bouldin": round(db, 3),
                    "DBCV": None,
                    "Num_Clusters": params['n_clusters']
                })
            except Exception as e:
                print(f"‚ùå KMeans failed: {e}")

# Save results
df = pd.DataFrame(results)
df.to_csv("cluster_evaluation_results.txt", index=False)
print("\n‚úÖ Saved to 'cluster_evaluation_results.csv'")
print(df.sort_values(by="Silhouette", ascending=False).head(10))
